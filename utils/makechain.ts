import { OpenAIChat } from 'langchain/llms';
import { LLMChain, ChatVectorDBQAChain, loadQAChain } from 'langchain/chains';
import { PineconeStore } from 'langchain/vectorstores';
import { PromptTemplate } from 'langchain/prompts';
import { CallbackManager } from 'langchain/callbacks';
import { Configuration } from 'openai';

const CONDENSE_PROMPT =
  PromptTemplate.fromTemplate(`鉴于以下对话和追问，将追问的问题改写为一个单独的问题。
对话的历史记录:
{chat_history}
问题: {question}
独立问题:`);

// const CONDENSE_PROMPT =
//   PromptTemplate.fromTemplate(`
// 问题: {question}
// `);

const QA_PROMPT = PromptTemplate.fromTemplate(
  `您是提供员工保险理赔的AI向导。以下是保险手册的一部分相关问题的材料。根据以下提供的上下文生成答案。
  您只能引用以下的文字内容。不要编造答案。
  如果您在下面的文字中找不到答案，只需回答："我不确定。"
  不要试图编造答案。
问题: {question}
=========
{context}
=========
`);

export const makeChain = (
  vectorstore: PineconeStore,
  onTokenStream?: (token: string) => void,
) => {
  const questionGenerator = new LLMChain({
    llm: new OpenAIChat({ temperature: 0 }, new Configuration({
      basePath:process.env.OPENAI_API_BASE_URL,
      apiKey:process.env.OPENAI_API_KEY
    })),
    prompt: CONDENSE_PROMPT,
  });
  const docChain = loadQAChain(
    new OpenAIChat({
      temperature: 0,
      modelName: 'gpt-3.5-turbo', //change this to older versions (e.g. gpt-3.5-turbo) if you don't have access to gpt-4
      streaming: Boolean(onTokenStream),
      callbackManager: onTokenStream
        ? CallbackManager.fromHandlers({
          async handleLLMNewToken(token) {
            onTokenStream(token);
            //console.log(token);
          },
        })
        : Object(),
    }, new Configuration({
      basePath:process.env.OPENAI_API_BASE_URL,
      apiKey:process.env.OPENAI_API_KEY
    })),
    { prompt: QA_PROMPT },
  );

  return new ChatVectorDBQAChain({
    vectorstore,
    combineDocumentsChain: docChain,
    questionGeneratorChain: questionGenerator,
    returnSourceDocuments: false,
    k: 2, //number of source documents to return
  });
};
